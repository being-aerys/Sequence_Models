{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM in PyTorch for Character Generation\n",
    "\n",
    "#on top of the code by LeanManager for my personal understanding. Original copyright for LSTM belongs to Hochreiter & Schmidhuber. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(tuple(set(text)))'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open text file and read in data as `text`\n",
    "\n",
    "#with open('anna.txt', 'r') as f:\n",
    "with open('anna.txt', 'r') as f:\n",
    "\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "'''set(\"some_string\")'''\n",
    "#creates a dictionary of unique values from the items of the iterables passed to it.\n",
    "#text = \"Aashish\"\n",
    "#print(set(text)) prints {'s', 'A', 'h', 'i'}\n",
    "\n",
    "#print(set(text))\n",
    "\n",
    "'''print(tuple(set(text)))'''\n",
    "#converts the dictionary into a comma-separated tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictonaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "\n",
    "\n",
    "\n",
    "character_vocabulary = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(character_vocabulary))\n",
    "# creates {0: ')', 1: 'y', 2: 'a', 3: '/', 4: 'B', 5: 'f', 6: 'U', 7: 'X', .......and so on }\n",
    "\n",
    "#just reverse the above key-value pairs \n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "#convert the whole text into the corresponding integer value\n",
    "each_char_in_encodeed_integer_form = np.array([char2int[ch] for ch in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "\n",
    "To train on this data, we also want to create mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "In this example, we'll take the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `no_of_seqs_per_batch` (also refered to as \"batch size\" in other places). Each of those sequences will be `no_of_timesteps_per_sequence` long.\n",
    "\n",
    "### Creating Batches\n",
    "\n",
    "**1. The first thing we need to do is discard some of the text so we only have completely full batches. **\n",
    "\n",
    "Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the total number of batches, $K$, we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n",
    "\n",
    "**2. After that, we need to split `arr` into $N$ sequences. ** \n",
    "\n",
    "You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences, so let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n",
    "\n",
    "**3. Now that we have this array, we can iterate through it to get our batches. **\n",
    "\n",
    "The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `no_of_timesteps_per_sequence`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. The way I like to do this window is use `range` to take steps of size `no_of_timesteps_per_sequence` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `no_of_timesteps_per_sequence` wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(array_of_characters, no_of_seqs_per_batch, no_of_timesteps_per_sequence):# _, 10, 50\n",
    "    '''Create a generator that returns batches of size\n",
    "       no_of_seqs_per_batch x no_of_timesteps_per_sequence from array_of_characters.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       array_of_characters: Array you want to make batches from\n",
    "       no_of_seqs_per_batch: Batch size, the number of sequences per batch\n",
    "       no_of_timesteps_per_sequence: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    batch_size = no_of_seqs_per_batch * no_of_timesteps_per_sequence\n",
    "    no_of_batches = len(array_of_characters)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    array_of_characters = array_of_characters[:no_of_batches * batch_size]\n",
    "    \n",
    "    # Reshape into no_of_seqs_per_batch rows (In our example, 2)\n",
    "    array_of_characters = array_of_characters.reshape((no_of_seqs_per_batch, -1))\n",
    "       \n",
    "    for n in range(0, array_of_characters.shape[1], no_of_timesteps_per_sequence): #simple way to get the batches\n",
    "        \n",
    "        # The features\n",
    "        x = array_of_characters[:, n:n+no_of_timesteps_per_sequence]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], array_of_characters[:, n+no_of_timesteps_per_sequence] #sweet\n",
    "#             print(x)\n",
    "#             print(y)\n",
    "            \n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], array_of_characters[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Char_Generation(nn.Module):\n",
    "    \n",
    "    def __init__(self, unique_vocabulary_tokens, no_of_timesteps_per_sequence=100, no_of_hidden_nodes_in_each_lstm_cell=256, no_of_stacked_lstm_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.no_of_stacked_lstm_layers = no_of_stacked_lstm_layers\n",
    "        self.no_of_hidden_nodes_in_each_lstm_cell = no_of_hidden_nodes_in_each_lstm_cell\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating character dictionaries\n",
    "        self.character_vocabulary = unique_vocabulary_tokens\n",
    "        \n",
    "        self.int2char = dict(enumerate(self.character_vocabulary))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## Define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.character_vocabulary), no_of_hidden_nodes_in_each_lstm_cell, no_of_stacked_lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## Define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(no_of_hidden_nodes_in_each_lstm_cell, len(self.character_vocabulary))\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden_and_cell_states):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden and cell states `hidden_and_cell_states`. '''\n",
    "        \n",
    "        #Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hidden_and_cell_states)\n",
    "        \n",
    "        ## Pass x through the dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.no_of_hidden_nodes_in_each_lstm_cell)\n",
    "        \n",
    "        ## Put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        \n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        #The output of our RNN is from a fully-connected layer and it outputs a distribution of next-character \n",
    "        #scores. To actually get the next character, we apply a softmax function, \n",
    "        #which gives us a probability distribution that we can then sample to predict the next character\n",
    "        \n",
    "        \n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1) \n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encoder(x, len(self.character_vocabulary))\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.character_vocabulary))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        \n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, no_of_seqs_per_batch):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x no_of_seqs_per_batch x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.no_of_stacked_lstm_layers, no_of_seqs_per_batch, self.no_of_hidden_nodes_in_each_lstm_cell).zero_(),\n",
    "                weight.new(self.no_of_stacked_lstm_layers, no_of_seqs_per_batch, self.no_of_hidden_nodes_in_each_lstm_cell).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the `predict`  function\n",
    "\n",
    "The output of our RNN is from a fully-connected layer and it outputs a **distribution of next-character scores**.\n",
    "\n",
    "To actually get the next character, we apply a softmax function, which gives us a *probability* distribution that we can then sample to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, data, epochs=10, no_of_seqs_per_batch=10, no_of_timesteps_per_sequence=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        no_of_seqs_per_batch: Number of mini-sequences per mini-batch, aka batch size\n",
    "        no_of_timesteps_per_sequence: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    network.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        network.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(network.character_vocabulary)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        h = network.init_hidden(no_of_seqs_per_batch)\n",
    "        \n",
    "        for x, y in get_batches(data, no_of_seqs_per_batch, no_of_timesteps_per_sequence):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encoder(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            network.zero_grad()\n",
    "            \n",
    "            output, h = network.forward(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(no_of_seqs_per_batch*no_of_timesteps_per_sequence).type(torch.cuda.LongTensor))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = network.init_hidden(no_of_seqs_per_batch)\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batches(val_data, no_of_seqs_per_batch, no_of_timesteps_per_sequence):\n",
    "                    \n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encoder(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = network.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(no_of_seqs_per_batch*no_of_timesteps_per_sequence).type(torch.cuda.LongTensor))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'network' in locals(): # function returns a dictionary containing the variables defined in the local namespace\n",
    "    del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Char_Generation(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "\n",
    "network = LSTM_Char_Generation(character_vocabulary, no_of_hidden_nodes_in_each_lstm_cell=512, no_of_stacked_lstm_layers=2)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40... Step: 10... Loss: 1.8579... Val Loss: 1.9519\n",
      "Epoch: 1/40... Step: 20... Loss: 1.8599... Val Loss: 1.9406\n",
      "Epoch: 1/40... Step: 30... Loss: 1.8395... Val Loss: 1.9417\n",
      "Epoch: 1/40... Step: 40... Loss: 1.8270... Val Loss: 1.9353\n",
      "Epoch: 1/40... Step: 50... Loss: 1.8468... Val Loss: 1.9408\n",
      "Epoch: 1/40... Step: 60... Loss: 1.7811... Val Loss: 1.9303\n",
      "Epoch: 1/40... Step: 70... Loss: 1.7993... Val Loss: 1.9244\n",
      "Epoch: 1/40... Step: 80... Loss: 1.7767... Val Loss: 1.9205\n",
      "Epoch: 1/40... Step: 90... Loss: 1.8129... Val Loss: 1.9114\n",
      "Epoch: 1/40... Step: 100... Loss: 1.7887... Val Loss: 1.9194\n",
      "Epoch: 1/40... Step: 110... Loss: 1.7753... Val Loss: 1.9102\n",
      "Epoch: 1/40... Step: 120... Loss: 1.7621... Val Loss: 1.9096\n",
      "Epoch: 1/40... Step: 130... Loss: 1.8190... Val Loss: 1.9159\n",
      "Epoch: 2/40... Step: 140... Loss: 1.8436... Val Loss: 1.9283\n",
      "Epoch: 2/40... Step: 150... Loss: 1.8034... Val Loss: 1.9179\n",
      "Epoch: 2/40... Step: 160... Loss: 1.8296... Val Loss: 1.9112\n",
      "Epoch: 2/40... Step: 170... Loss: 1.8386... Val Loss: 1.9084\n",
      "Epoch: 2/40... Step: 180... Loss: 1.7958... Val Loss: 1.9156\n",
      "Epoch: 2/40... Step: 190... Loss: 1.7567... Val Loss: 1.9081\n",
      "Epoch: 2/40... Step: 200... Loss: 1.7637... Val Loss: 1.9116\n",
      "Epoch: 2/40... Step: 210... Loss: 1.7944... Val Loss: 1.9069\n",
      "Epoch: 2/40... Step: 220... Loss: 1.7766... Val Loss: 1.9039\n",
      "Epoch: 2/40... Step: 230... Loss: 1.8000... Val Loss: 1.9071\n",
      "Epoch: 2/40... Step: 240... Loss: 1.8019... Val Loss: 1.9084\n",
      "Epoch: 2/40... Step: 250... Loss: 1.7712... Val Loss: 1.9025\n",
      "Epoch: 2/40... Step: 260... Loss: 1.7427... Val Loss: 1.9042\n",
      "Epoch: 2/40... Step: 270... Loss: 1.7876... Val Loss: 1.9063\n",
      "Epoch: 3/40... Step: 280... Loss: 1.7866... Val Loss: 1.9031\n",
      "Epoch: 3/40... Step: 290... Loss: 1.8139... Val Loss: 1.9044\n",
      "Epoch: 3/40... Step: 300... Loss: 1.7787... Val Loss: 1.8994\n",
      "Epoch: 3/40... Step: 310... Loss: 1.7726... Val Loss: 1.9017\n",
      "Epoch: 3/40... Step: 320... Loss: 1.7559... Val Loss: 1.8991\n",
      "Epoch: 3/40... Step: 330... Loss: 1.7534... Val Loss: 1.8979\n",
      "Epoch: 3/40... Step: 340... Loss: 1.8128... Val Loss: 1.8971\n",
      "Epoch: 3/40... Step: 350... Loss: 1.7680... Val Loss: 1.8968\n",
      "Epoch: 3/40... Step: 360... Loss: 1.7223... Val Loss: 1.8895\n",
      "Epoch: 3/40... Step: 370... Loss: 1.7723... Val Loss: 1.8936\n",
      "Epoch: 3/40... Step: 380... Loss: 1.7775... Val Loss: 1.8913\n",
      "Epoch: 3/40... Step: 390... Loss: 1.7576... Val Loss: 1.8862\n",
      "Epoch: 3/40... Step: 400... Loss: 1.7405... Val Loss: 1.8934\n",
      "Epoch: 3/40... Step: 410... Loss: 1.7654... Val Loss: 1.8890\n",
      "Epoch: 4/40... Step: 420... Loss: 1.7825... Val Loss: 1.8834\n",
      "Epoch: 4/40... Step: 430... Loss: 1.7723... Val Loss: 1.8856\n",
      "Epoch: 4/40... Step: 440... Loss: 1.7726... Val Loss: 1.8855\n",
      "Epoch: 4/40... Step: 450... Loss: 1.7256... Val Loss: 1.8833\n",
      "Epoch: 4/40... Step: 460... Loss: 1.7279... Val Loss: 1.8902\n",
      "Epoch: 4/40... Step: 470... Loss: 1.7946... Val Loss: 1.8880\n",
      "Epoch: 4/40... Step: 480... Loss: 1.7717... Val Loss: 1.8802\n",
      "Epoch: 4/40... Step: 490... Loss: 1.7812... Val Loss: 1.8774\n",
      "Epoch: 4/40... Step: 500... Loss: 1.7763... Val Loss: 1.8730\n",
      "Epoch: 4/40... Step: 510... Loss: 1.7616... Val Loss: 1.8785\n",
      "Epoch: 4/40... Step: 520... Loss: 1.7742... Val Loss: 1.8796\n",
      "Epoch: 4/40... Step: 530... Loss: 1.7548... Val Loss: 1.8771\n",
      "Epoch: 4/40... Step: 540... Loss: 1.7378... Val Loss: 1.8843\n",
      "Epoch: 4/40... Step: 550... Loss: 1.7863... Val Loss: 1.8761\n",
      "Epoch: 5/40... Step: 560... Loss: 1.7519... Val Loss: 1.8785\n",
      "Epoch: 5/40... Step: 570... Loss: 1.7484... Val Loss: 1.8824\n",
      "Epoch: 5/40... Step: 580... Loss: 1.7424... Val Loss: 1.8742\n",
      "Epoch: 5/40... Step: 590... Loss: 1.7516... Val Loss: 1.8832\n",
      "Epoch: 5/40... Step: 600... Loss: 1.7368... Val Loss: 1.8781\n",
      "Epoch: 5/40... Step: 610... Loss: 1.7282... Val Loss: 1.8707\n",
      "Epoch: 5/40... Step: 620... Loss: 1.7422... Val Loss: 1.8838\n",
      "Epoch: 5/40... Step: 630... Loss: 1.7746... Val Loss: 1.8742\n",
      "Epoch: 5/40... Step: 640... Loss: 1.7351... Val Loss: 1.8655\n",
      "Epoch: 5/40... Step: 650... Loss: 1.7440... Val Loss: 1.8666\n",
      "Epoch: 5/40... Step: 660... Loss: 1.7116... Val Loss: 1.8693\n",
      "Epoch: 5/40... Step: 670... Loss: 1.7569... Val Loss: 1.8673\n",
      "Epoch: 5/40... Step: 680... Loss: 1.7481... Val Loss: 1.8710\n",
      "Epoch: 5/40... Step: 690... Loss: 1.7316... Val Loss: 1.8693\n",
      "Epoch: 6/40... Step: 700... Loss: 1.7289... Val Loss: 1.8738\n",
      "Epoch: 6/40... Step: 710... Loss: 1.7391... Val Loss: 1.8683\n",
      "Epoch: 6/40... Step: 720... Loss: 1.7337... Val Loss: 1.8616\n",
      "Epoch: 6/40... Step: 730... Loss: 1.7505... Val Loss: 1.8640\n",
      "Epoch: 6/40... Step: 740... Loss: 1.7206... Val Loss: 1.8617\n",
      "Epoch: 6/40... Step: 750... Loss: 1.7087... Val Loss: 1.8647\n",
      "Epoch: 6/40... Step: 760... Loss: 1.7463... Val Loss: 1.8701\n",
      "Epoch: 6/40... Step: 770... Loss: 1.7224... Val Loss: 1.8598\n",
      "Epoch: 6/40... Step: 780... Loss: 1.7210... Val Loss: 1.8636\n",
      "Epoch: 6/40... Step: 790... Loss: 1.7156... Val Loss: 1.8671\n",
      "Epoch: 6/40... Step: 800... Loss: 1.7327... Val Loss: 1.8595\n",
      "Epoch: 6/40... Step: 810... Loss: 1.7259... Val Loss: 1.8566\n",
      "Epoch: 6/40... Step: 820... Loss: 1.6965... Val Loss: 1.8576\n",
      "Epoch: 6/40... Step: 830... Loss: 1.7470... Val Loss: 1.8544\n",
      "Epoch: 7/40... Step: 840... Loss: 1.7035... Val Loss: 1.8496\n",
      "Epoch: 7/40... Step: 850... Loss: 1.7141... Val Loss: 1.8531\n",
      "Epoch: 7/40... Step: 860... Loss: 1.7056... Val Loss: 1.8598\n",
      "Epoch: 7/40... Step: 870... Loss: 1.7307... Val Loss: 1.8562\n",
      "Epoch: 7/40... Step: 880... Loss: 1.7292... Val Loss: 1.8525\n",
      "Epoch: 7/40... Step: 890... Loss: 1.7213... Val Loss: 1.8488\n",
      "Epoch: 7/40... Step: 900... Loss: 1.7141... Val Loss: 1.8547\n",
      "Epoch: 7/40... Step: 910... Loss: 1.6806... Val Loss: 1.8488\n",
      "Epoch: 7/40... Step: 920... Loss: 1.7193... Val Loss: 1.8425\n",
      "Epoch: 7/40... Step: 930... Loss: 1.7073... Val Loss: 1.8447\n",
      "Epoch: 7/40... Step: 940... Loss: 1.7056... Val Loss: 1.8444\n",
      "Epoch: 7/40... Step: 950... Loss: 1.7192... Val Loss: 1.8420\n",
      "Epoch: 7/40... Step: 960... Loss: 1.7294... Val Loss: 1.8398\n",
      "Epoch: 7/40... Step: 970... Loss: 1.7369... Val Loss: 1.8467\n",
      "Epoch: 8/40... Step: 980... Loss: 1.7075... Val Loss: 1.8463\n",
      "Epoch: 8/40... Step: 990... Loss: 1.7158... Val Loss: 1.8497\n",
      "Epoch: 8/40... Step: 1000... Loss: 1.7031... Val Loss: 1.8417\n",
      "Epoch: 8/40... Step: 1010... Loss: 1.7637... Val Loss: 1.8368\n",
      "Epoch: 8/40... Step: 1020... Loss: 1.7159... Val Loss: 1.8421\n",
      "Epoch: 8/40... Step: 1030... Loss: 1.6958... Val Loss: 1.8408\n",
      "Epoch: 8/40... Step: 1040... Loss: 1.7086... Val Loss: 1.8439\n",
      "Epoch: 8/40... Step: 1050... Loss: 1.6995... Val Loss: 1.8386\n",
      "Epoch: 8/40... Step: 1060... Loss: 1.6952... Val Loss: 1.8382\n",
      "Epoch: 8/40... Step: 1070... Loss: 1.7220... Val Loss: 1.8385\n",
      "Epoch: 8/40... Step: 1080... Loss: 1.7159... Val Loss: 1.8382\n",
      "Epoch: 8/40... Step: 1090... Loss: 1.7008... Val Loss: 1.8287\n",
      "Epoch: 8/40... Step: 1100... Loss: 1.6847... Val Loss: 1.8372\n",
      "Epoch: 8/40... Step: 1110... Loss: 1.7130... Val Loss: 1.8391\n",
      "Epoch: 9/40... Step: 1120... Loss: 1.7172... Val Loss: 1.8331\n",
      "Epoch: 9/40... Step: 1130... Loss: 1.6987... Val Loss: 1.8425\n",
      "Epoch: 9/40... Step: 1140... Loss: 1.7166... Val Loss: 1.8304\n",
      "Epoch: 9/40... Step: 1150... Loss: 1.7289... Val Loss: 1.8279\n",
      "Epoch: 9/40... Step: 1160... Loss: 1.6804... Val Loss: 1.8368\n",
      "Epoch: 9/40... Step: 1170... Loss: 1.6927... Val Loss: 1.8403\n",
      "Epoch: 9/40... Step: 1180... Loss: 1.6934... Val Loss: 1.8356\n",
      "Epoch: 9/40... Step: 1190... Loss: 1.7178... Val Loss: 1.8311\n",
      "Epoch: 9/40... Step: 1200... Loss: 1.6796... Val Loss: 1.8272\n",
      "Epoch: 9/40... Step: 1210... Loss: 1.6983... Val Loss: 1.8289\n",
      "Epoch: 9/40... Step: 1220... Loss: 1.6860... Val Loss: 1.8271\n",
      "Epoch: 9/40... Step: 1230... Loss: 1.6899... Val Loss: 1.8236\n",
      "Epoch: 9/40... Step: 1240... Loss: 1.6871... Val Loss: 1.8230\n",
      "Epoch: 9/40... Step: 1250... Loss: 1.7023... Val Loss: 1.8351\n",
      "Epoch: 10/40... Step: 1260... Loss: 1.7013... Val Loss: 1.8293\n",
      "Epoch: 10/40... Step: 1270... Loss: 1.6763... Val Loss: 1.8233\n",
      "Epoch: 10/40... Step: 1280... Loss: 1.7054... Val Loss: 1.8217\n",
      "Epoch: 10/40... Step: 1290... Loss: 1.7034... Val Loss: 1.8245\n",
      "Epoch: 10/40... Step: 1300... Loss: 1.6843... Val Loss: 1.8215\n",
      "Epoch: 10/40... Step: 1310... Loss: 1.6977... Val Loss: 1.8197\n",
      "Epoch: 10/40... Step: 1320... Loss: 1.6749... Val Loss: 1.8234\n",
      "Epoch: 10/40... Step: 1330... Loss: 1.6808... Val Loss: 1.8203\n",
      "Epoch: 10/40... Step: 1340... Loss: 1.6709... Val Loss: 1.8211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/40... Step: 1350... Loss: 1.6650... Val Loss: 1.8235\n",
      "Epoch: 10/40... Step: 1360... Loss: 1.6772... Val Loss: 1.8224\n",
      "Epoch: 10/40... Step: 1370... Loss: 1.6645... Val Loss: 1.8164\n",
      "Epoch: 10/40... Step: 1380... Loss: 1.7092... Val Loss: 1.8140\n",
      "Epoch: 10/40... Step: 1390... Loss: 1.6977... Val Loss: 1.8139\n",
      "Epoch: 11/40... Step: 1400... Loss: 1.7097... Val Loss: 1.8150\n",
      "Epoch: 11/40... Step: 1410... Loss: 1.7223... Val Loss: 1.8138\n",
      "Epoch: 11/40... Step: 1420... Loss: 1.6980... Val Loss: 1.8176\n",
      "Epoch: 11/40... Step: 1430... Loss: 1.6792... Val Loss: 1.8192\n",
      "Epoch: 11/40... Step: 1440... Loss: 1.7123... Val Loss: 1.8141\n",
      "Epoch: 11/40... Step: 1450... Loss: 1.6461... Val Loss: 1.8150\n",
      "Epoch: 11/40... Step: 1460... Loss: 1.6702... Val Loss: 1.8124\n",
      "Epoch: 11/40... Step: 1470... Loss: 1.6558... Val Loss: 1.8087\n",
      "Epoch: 11/40... Step: 1480... Loss: 1.6707... Val Loss: 1.8121\n",
      "Epoch: 11/40... Step: 1490... Loss: 1.6728... Val Loss: 1.8113\n",
      "Epoch: 11/40... Step: 1500... Loss: 1.6589... Val Loss: 1.8036\n",
      "Epoch: 11/40... Step: 1510... Loss: 1.6376... Val Loss: 1.8037\n",
      "Epoch: 11/40... Step: 1520... Loss: 1.6837... Val Loss: 1.8043\n",
      "Epoch: 12/40... Step: 1530... Loss: 1.7465... Val Loss: 1.8079\n",
      "Epoch: 12/40... Step: 1540... Loss: 1.6812... Val Loss: 1.8087\n",
      "Epoch: 12/40... Step: 1550... Loss: 1.7007... Val Loss: 1.8046\n",
      "Epoch: 12/40... Step: 1560... Loss: 1.7085... Val Loss: 1.8055\n",
      "Epoch: 12/40... Step: 1570... Loss: 1.6674... Val Loss: 1.8062\n",
      "Epoch: 12/40... Step: 1580... Loss: 1.6384... Val Loss: 1.8059\n",
      "Epoch: 12/40... Step: 1590... Loss: 1.6352... Val Loss: 1.8173\n",
      "Epoch: 12/40... Step: 1600... Loss: 1.6760... Val Loss: 1.8091\n",
      "Epoch: 12/40... Step: 1610... Loss: 1.6561... Val Loss: 1.8038\n",
      "Epoch: 12/40... Step: 1620... Loss: 1.6741... Val Loss: 1.8024\n",
      "Epoch: 12/40... Step: 1630... Loss: 1.6883... Val Loss: 1.8044\n",
      "Epoch: 12/40... Step: 1640... Loss: 1.6662... Val Loss: 1.8017\n",
      "Epoch: 12/40... Step: 1650... Loss: 1.6310... Val Loss: 1.7970\n",
      "Epoch: 12/40... Step: 1660... Loss: 1.6723... Val Loss: 1.8016\n",
      "Epoch: 13/40... Step: 1670... Loss: 1.6602... Val Loss: 1.8078\n",
      "Epoch: 13/40... Step: 1680... Loss: 1.6867... Val Loss: 1.8110\n",
      "Epoch: 13/40... Step: 1690... Loss: 1.6554... Val Loss: 1.8019\n",
      "Epoch: 13/40... Step: 1700... Loss: 1.6650... Val Loss: 1.8003\n",
      "Epoch: 13/40... Step: 1710... Loss: 1.6405... Val Loss: 1.7956\n",
      "Epoch: 13/40... Step: 1720... Loss: 1.6395... Val Loss: 1.7960\n",
      "Epoch: 13/40... Step: 1730... Loss: 1.6833... Val Loss: 1.7937\n",
      "Epoch: 13/40... Step: 1740... Loss: 1.6509... Val Loss: 1.7959\n",
      "Epoch: 13/40... Step: 1750... Loss: 1.6181... Val Loss: 1.7946\n",
      "Epoch: 13/40... Step: 1760... Loss: 1.6733... Val Loss: 1.8080\n",
      "Epoch: 13/40... Step: 1770... Loss: 1.6696... Val Loss: 1.8029\n",
      "Epoch: 13/40... Step: 1780... Loss: 1.6406... Val Loss: 1.7949\n",
      "Epoch: 13/40... Step: 1790... Loss: 1.6324... Val Loss: 1.7958\n",
      "Epoch: 13/40... Step: 1800... Loss: 1.6538... Val Loss: 1.7975\n",
      "Epoch: 14/40... Step: 1810... Loss: 1.6686... Val Loss: 1.7912\n",
      "Epoch: 14/40... Step: 1820... Loss: 1.6613... Val Loss: 1.7972\n",
      "Epoch: 14/40... Step: 1830... Loss: 1.6674... Val Loss: 1.7904\n",
      "Epoch: 14/40... Step: 1840... Loss: 1.6202... Val Loss: 1.7942\n",
      "Epoch: 14/40... Step: 1850... Loss: 1.6146... Val Loss: 1.7927\n",
      "Epoch: 14/40... Step: 1860... Loss: 1.6707... Val Loss: 1.7920\n",
      "Epoch: 14/40... Step: 1870... Loss: 1.6643... Val Loss: 1.7902\n",
      "Epoch: 14/40... Step: 1880... Loss: 1.6694... Val Loss: 1.7831\n",
      "Epoch: 14/40... Step: 1890... Loss: 1.6755... Val Loss: 1.7823\n",
      "Epoch: 14/40... Step: 1900... Loss: 1.6607... Val Loss: 1.7845\n",
      "Epoch: 14/40... Step: 1910... Loss: 1.6629... Val Loss: 1.7939\n",
      "Epoch: 14/40... Step: 1920... Loss: 1.6407... Val Loss: 1.7861\n",
      "Epoch: 14/40... Step: 1930... Loss: 1.6150... Val Loss: 1.7879\n",
      "Epoch: 14/40... Step: 1940... Loss: 1.6829... Val Loss: 1.7941\n",
      "Epoch: 15/40... Step: 1950... Loss: 1.6555... Val Loss: 1.7869\n",
      "Epoch: 15/40... Step: 1960... Loss: 1.6372... Val Loss: 1.7847\n",
      "Epoch: 15/40... Step: 1970... Loss: 1.6120... Val Loss: 1.7792\n",
      "Epoch: 15/40... Step: 1980... Loss: 1.6382... Val Loss: 1.7798\n",
      "Epoch: 15/40... Step: 1990... Loss: 1.6354... Val Loss: 1.7792\n",
      "Epoch: 15/40... Step: 2000... Loss: 1.6153... Val Loss: 1.7763\n",
      "Epoch: 15/40... Step: 2010... Loss: 1.6490... Val Loss: 1.7765\n",
      "Epoch: 15/40... Step: 2020... Loss: 1.6635... Val Loss: 1.7747\n",
      "Epoch: 15/40... Step: 2030... Loss: 1.6394... Val Loss: 1.7761\n",
      "Epoch: 15/40... Step: 2040... Loss: 1.6460... Val Loss: 1.7735\n",
      "Epoch: 15/40... Step: 2050... Loss: 1.6187... Val Loss: 1.7736\n",
      "Epoch: 15/40... Step: 2060... Loss: 1.6510... Val Loss: 1.7780\n",
      "Epoch: 15/40... Step: 2070... Loss: 1.6429... Val Loss: 1.7765\n",
      "Epoch: 15/40... Step: 2080... Loss: 1.6289... Val Loss: 1.7793\n",
      "Epoch: 16/40... Step: 2090... Loss: 1.6344... Val Loss: 1.7750\n",
      "Epoch: 16/40... Step: 2100... Loss: 1.6386... Val Loss: 1.7727\n",
      "Epoch: 16/40... Step: 2110... Loss: 1.6146... Val Loss: 1.7712\n",
      "Epoch: 16/40... Step: 2120... Loss: 1.6430... Val Loss: 1.7781\n",
      "Epoch: 16/40... Step: 2130... Loss: 1.6198... Val Loss: 1.7752\n",
      "Epoch: 16/40... Step: 2140... Loss: 1.6028... Val Loss: 1.7720\n",
      "Epoch: 16/40... Step: 2150... Loss: 1.6527... Val Loss: 1.7757\n",
      "Epoch: 16/40... Step: 2160... Loss: 1.6286... Val Loss: 1.7710\n",
      "Epoch: 16/40... Step: 2170... Loss: 1.6133... Val Loss: 1.7734\n",
      "Epoch: 16/40... Step: 2180... Loss: 1.6107... Val Loss: 1.7671\n",
      "Epoch: 16/40... Step: 2190... Loss: 1.6345... Val Loss: 1.7748\n",
      "Epoch: 16/40... Step: 2200... Loss: 1.6156... Val Loss: 1.7697\n",
      "Epoch: 16/40... Step: 2210... Loss: 1.5841... Val Loss: 1.7713\n",
      "Epoch: 16/40... Step: 2220... Loss: 1.6498... Val Loss: 1.7703\n",
      "Epoch: 17/40... Step: 2230... Loss: 1.5946... Val Loss: 1.7647\n",
      "Epoch: 17/40... Step: 2240... Loss: 1.6109... Val Loss: 1.7725\n",
      "Epoch: 17/40... Step: 2250... Loss: 1.6089... Val Loss: 1.7609\n",
      "Epoch: 17/40... Step: 2260... Loss: 1.6314... Val Loss: 1.7702\n",
      "Epoch: 17/40... Step: 2270... Loss: 1.6244... Val Loss: 1.7669\n",
      "Epoch: 17/40... Step: 2280... Loss: 1.6261... Val Loss: 1.7714\n",
      "Epoch: 17/40... Step: 2290... Loss: 1.6194... Val Loss: 1.7719\n",
      "Epoch: 17/40... Step: 2300... Loss: 1.5810... Val Loss: 1.7677\n",
      "Epoch: 17/40... Step: 2310... Loss: 1.6213... Val Loss: 1.7681\n",
      "Epoch: 17/40... Step: 2320... Loss: 1.6065... Val Loss: 1.7626\n",
      "Epoch: 17/40... Step: 2330... Loss: 1.6041... Val Loss: 1.7622\n",
      "Epoch: 17/40... Step: 2340... Loss: 1.6182... Val Loss: 1.7646\n",
      "Epoch: 17/40... Step: 2350... Loss: 1.6115... Val Loss: 1.7618\n",
      "Epoch: 17/40... Step: 2360... Loss: 1.6381... Val Loss: 1.7591\n",
      "Epoch: 18/40... Step: 2370... Loss: 1.6174... Val Loss: 1.7614\n",
      "Epoch: 18/40... Step: 2380... Loss: 1.6180... Val Loss: 1.7610\n",
      "Epoch: 18/40... Step: 2390... Loss: 1.6067... Val Loss: 1.7591\n",
      "Epoch: 18/40... Step: 2400... Loss: 1.6561... Val Loss: 1.7647\n",
      "Epoch: 18/40... Step: 2410... Loss: 1.6322... Val Loss: 1.7583\n",
      "Epoch: 18/40... Step: 2420... Loss: 1.6154... Val Loss: 1.7587\n",
      "Epoch: 18/40... Step: 2430... Loss: 1.6073... Val Loss: 1.7619\n",
      "Epoch: 18/40... Step: 2440... Loss: 1.5852... Val Loss: 1.7630\n",
      "Epoch: 18/40... Step: 2450... Loss: 1.5977... Val Loss: 1.7597\n",
      "Epoch: 18/40... Step: 2460... Loss: 1.6170... Val Loss: 1.7539\n",
      "Epoch: 18/40... Step: 2470... Loss: 1.6090... Val Loss: 1.7609\n",
      "Epoch: 18/40... Step: 2480... Loss: 1.5917... Val Loss: 1.7602\n",
      "Epoch: 18/40... Step: 2490... Loss: 1.6001... Val Loss: 1.7665\n",
      "Epoch: 18/40... Step: 2500... Loss: 1.6125... Val Loss: 1.7603\n",
      "Epoch: 19/40... Step: 2510... Loss: 1.6273... Val Loss: 1.7568\n",
      "Epoch: 19/40... Step: 2520... Loss: 1.6029... Val Loss: 1.7502\n",
      "Epoch: 19/40... Step: 2530... Loss: 1.6270... Val Loss: 1.7500\n",
      "Epoch: 19/40... Step: 2540... Loss: 1.6293... Val Loss: 1.7524\n",
      "Epoch: 19/40... Step: 2550... Loss: 1.5911... Val Loss: 1.7502\n",
      "Epoch: 19/40... Step: 2560... Loss: 1.6033... Val Loss: 1.7547\n",
      "Epoch: 19/40... Step: 2570... Loss: 1.6007... Val Loss: 1.7475\n",
      "Epoch: 19/40... Step: 2580... Loss: 1.6319... Val Loss: 1.7493\n",
      "Epoch: 19/40... Step: 2590... Loss: 1.5837... Val Loss: 1.7510\n",
      "Epoch: 19/40... Step: 2600... Loss: 1.6067... Val Loss: 1.7503\n",
      "Epoch: 19/40... Step: 2610... Loss: 1.5932... Val Loss: 1.7535\n",
      "Epoch: 19/40... Step: 2620... Loss: 1.5885... Val Loss: 1.7494\n",
      "Epoch: 19/40... Step: 2630... Loss: 1.5926... Val Loss: 1.7471\n",
      "Epoch: 19/40... Step: 2640... Loss: 1.6049... Val Loss: 1.7472\n",
      "Epoch: 20/40... Step: 2650... Loss: 1.6081... Val Loss: 1.7462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/40... Step: 2660... Loss: 1.5945... Val Loss: 1.7474\n",
      "Epoch: 20/40... Step: 2670... Loss: 1.6176... Val Loss: 1.7402\n",
      "Epoch: 20/40... Step: 2680... Loss: 1.6067... Val Loss: 1.7435\n",
      "Epoch: 20/40... Step: 2690... Loss: 1.5962... Val Loss: 1.7463\n",
      "Epoch: 20/40... Step: 2700... Loss: 1.6118... Val Loss: 1.7413\n",
      "Epoch: 20/40... Step: 2710... Loss: 1.5736... Val Loss: 1.7412\n",
      "Epoch: 20/40... Step: 2720... Loss: 1.5922... Val Loss: 1.7461\n",
      "Epoch: 20/40... Step: 2730... Loss: 1.5821... Val Loss: 1.7477\n",
      "Epoch: 20/40... Step: 2740... Loss: 1.5721... Val Loss: 1.7461\n",
      "Epoch: 20/40... Step: 2750... Loss: 1.5806... Val Loss: 1.7468\n",
      "Epoch: 20/40... Step: 2760... Loss: 1.5693... Val Loss: 1.7402\n",
      "Epoch: 20/40... Step: 2770... Loss: 1.6036... Val Loss: 1.7365\n",
      "Epoch: 20/40... Step: 2780... Loss: 1.6094... Val Loss: 1.7350\n",
      "Epoch: 21/40... Step: 2790... Loss: 1.6217... Val Loss: 1.7358\n",
      "Epoch: 21/40... Step: 2800... Loss: 1.6250... Val Loss: 1.7364\n",
      "Epoch: 21/40... Step: 2810... Loss: 1.6172... Val Loss: 1.7301\n",
      "Epoch: 21/40... Step: 2820... Loss: 1.5829... Val Loss: 1.7376\n",
      "Epoch: 21/40... Step: 2830... Loss: 1.6287... Val Loss: 1.7327\n",
      "Epoch: 21/40... Step: 2840... Loss: 1.5708... Val Loss: 1.7350\n",
      "Epoch: 21/40... Step: 2850... Loss: 1.5719... Val Loss: 1.7392\n",
      "Epoch: 21/40... Step: 2860... Loss: 1.5736... Val Loss: 1.7388\n",
      "Epoch: 21/40... Step: 2870... Loss: 1.5925... Val Loss: 1.7357\n",
      "Epoch: 21/40... Step: 2880... Loss: 1.5774... Val Loss: 1.7312\n",
      "Epoch: 21/40... Step: 2890... Loss: 1.5634... Val Loss: 1.7421\n",
      "Epoch: 21/40... Step: 2900... Loss: 1.5622... Val Loss: 1.7308\n",
      "Epoch: 21/40... Step: 2910... Loss: 1.6037... Val Loss: 1.7347\n",
      "Epoch: 22/40... Step: 2920... Loss: 1.6521... Val Loss: 1.7312\n",
      "Epoch: 22/40... Step: 2930... Loss: 1.5954... Val Loss: 1.7352\n",
      "Epoch: 22/40... Step: 2940... Loss: 1.6186... Val Loss: 1.7278\n",
      "Epoch: 22/40... Step: 2950... Loss: 1.6330... Val Loss: 1.7277\n",
      "Epoch: 22/40... Step: 2960... Loss: 1.5834... Val Loss: 1.7308\n",
      "Epoch: 22/40... Step: 2970... Loss: 1.5371... Val Loss: 1.7310\n",
      "Epoch: 22/40... Step: 2980... Loss: 1.5434... Val Loss: 1.7309\n",
      "Epoch: 22/40... Step: 2990... Loss: 1.5715... Val Loss: 1.7252\n",
      "Epoch: 22/40... Step: 3000... Loss: 1.5689... Val Loss: 1.7307\n",
      "Epoch: 22/40... Step: 3010... Loss: 1.5820... Val Loss: 1.7321\n",
      "Epoch: 22/40... Step: 3020... Loss: 1.5952... Val Loss: 1.7299\n",
      "Epoch: 22/40... Step: 3030... Loss: 1.5739... Val Loss: 1.7325\n",
      "Epoch: 22/40... Step: 3040... Loss: 1.5406... Val Loss: 1.7275\n",
      "Epoch: 22/40... Step: 3050... Loss: 1.5842... Val Loss: 1.7209\n",
      "Epoch: 23/40... Step: 3060... Loss: 1.5613... Val Loss: 1.7262\n",
      "Epoch: 23/40... Step: 3070... Loss: 1.5866... Val Loss: 1.7250\n",
      "Epoch: 23/40... Step: 3080... Loss: 1.5550... Val Loss: 1.7210\n",
      "Epoch: 23/40... Step: 3090... Loss: 1.5712... Val Loss: 1.7239\n",
      "Epoch: 23/40... Step: 3100... Loss: 1.5366... Val Loss: 1.7289\n",
      "Epoch: 23/40... Step: 3110... Loss: 1.5563... Val Loss: 1.7216\n",
      "Epoch: 23/40... Step: 3120... Loss: 1.6039... Val Loss: 1.7239\n",
      "Epoch: 23/40... Step: 3130... Loss: 1.5702... Val Loss: 1.7289\n",
      "Epoch: 23/40... Step: 3140... Loss: 1.5353... Val Loss: 1.7239\n",
      "Epoch: 23/40... Step: 3150... Loss: 1.5768... Val Loss: 1.7263\n",
      "Epoch: 23/40... Step: 3160... Loss: 1.5788... Val Loss: 1.7267\n",
      "Epoch: 23/40... Step: 3170... Loss: 1.5529... Val Loss: 1.7252\n",
      "Epoch: 23/40... Step: 3180... Loss: 1.5587... Val Loss: 1.7276\n",
      "Epoch: 23/40... Step: 3190... Loss: 1.5600... Val Loss: 1.7309\n",
      "Epoch: 24/40... Step: 3200... Loss: 1.5760... Val Loss: 1.7274\n",
      "Epoch: 24/40... Step: 3210... Loss: 1.5787... Val Loss: 1.7252\n",
      "Epoch: 24/40... Step: 3220... Loss: 1.5873... Val Loss: 1.7206\n",
      "Epoch: 24/40... Step: 3230... Loss: 1.5300... Val Loss: 1.7191\n",
      "Epoch: 24/40... Step: 3240... Loss: 1.5110... Val Loss: 1.7169\n",
      "Epoch: 24/40... Step: 3250... Loss: 1.5813... Val Loss: 1.7151\n",
      "Epoch: 24/40... Step: 3260... Loss: 1.5718... Val Loss: 1.7146\n",
      "Epoch: 24/40... Step: 3270... Loss: 1.5779... Val Loss: 1.7193\n",
      "Epoch: 24/40... Step: 3280... Loss: 1.5882... Val Loss: 1.7183\n",
      "Epoch: 24/40... Step: 3290... Loss: 1.5709... Val Loss: 1.7163\n",
      "Epoch: 24/40... Step: 3300... Loss: 1.5882... Val Loss: 1.7152\n",
      "Epoch: 24/40... Step: 3310... Loss: 1.5735... Val Loss: 1.7124\n",
      "Epoch: 24/40... Step: 3320... Loss: 1.5186... Val Loss: 1.7186\n",
      "Epoch: 24/40... Step: 3330... Loss: 1.6017... Val Loss: 1.7178\n",
      "Epoch: 25/40... Step: 3340... Loss: 1.5475... Val Loss: 1.7111\n",
      "Epoch: 25/40... Step: 3350... Loss: 1.5652... Val Loss: 1.7157\n",
      "Epoch: 25/40... Step: 3360... Loss: 1.5457... Val Loss: 1.7101\n",
      "Epoch: 25/40... Step: 3370... Loss: 1.5553... Val Loss: 1.7139\n",
      "Epoch: 25/40... Step: 3380... Loss: 1.5540... Val Loss: 1.7144\n",
      "Epoch: 25/40... Step: 3390... Loss: 1.5315... Val Loss: 1.7123\n",
      "Epoch: 25/40... Step: 3400... Loss: 1.5526... Val Loss: 1.7102\n",
      "Epoch: 25/40... Step: 3410... Loss: 1.5757... Val Loss: 1.7113\n",
      "Epoch: 25/40... Step: 3420... Loss: 1.5551... Val Loss: 1.7104\n",
      "Epoch: 25/40... Step: 3430... Loss: 1.5555... Val Loss: 1.7160\n",
      "Epoch: 25/40... Step: 3440... Loss: 1.5329... Val Loss: 1.7083\n",
      "Epoch: 25/40... Step: 3450... Loss: 1.5541... Val Loss: 1.7035\n",
      "Epoch: 25/40... Step: 3460... Loss: 1.5577... Val Loss: 1.7090\n",
      "Epoch: 25/40... Step: 3470... Loss: 1.5415... Val Loss: 1.7092\n",
      "Epoch: 26/40... Step: 3480... Loss: 1.5578... Val Loss: 1.7033\n",
      "Epoch: 26/40... Step: 3490... Loss: 1.5518... Val Loss: 1.7062\n",
      "Epoch: 26/40... Step: 3500... Loss: 1.5383... Val Loss: 1.6996\n",
      "Epoch: 26/40... Step: 3510... Loss: 1.5650... Val Loss: 1.7051\n",
      "Epoch: 26/40... Step: 3520... Loss: 1.5420... Val Loss: 1.7028\n",
      "Epoch: 26/40... Step: 3530... Loss: 1.5240... Val Loss: 1.7057\n",
      "Epoch: 26/40... Step: 3540... Loss: 1.5650... Val Loss: 1.7046\n",
      "Epoch: 26/40... Step: 3550... Loss: 1.5503... Val Loss: 1.7123\n",
      "Epoch: 26/40... Step: 3560... Loss: 1.5434... Val Loss: 1.7065\n",
      "Epoch: 26/40... Step: 3570... Loss: 1.5278... Val Loss: 1.7064\n",
      "Epoch: 26/40... Step: 3580... Loss: 1.5567... Val Loss: 1.7021\n",
      "Epoch: 26/40... Step: 3590... Loss: 1.5449... Val Loss: 1.6987\n",
      "Epoch: 26/40... Step: 3600... Loss: 1.5038... Val Loss: 1.7072\n",
      "Epoch: 26/40... Step: 3610... Loss: 1.5535... Val Loss: 1.6996\n",
      "Epoch: 27/40... Step: 3620... Loss: 1.5090... Val Loss: 1.6994\n",
      "Epoch: 27/40... Step: 3630... Loss: 1.5392... Val Loss: 1.7022\n",
      "Epoch: 27/40... Step: 3640... Loss: 1.5219... Val Loss: 1.6951\n",
      "Epoch: 27/40... Step: 3650... Loss: 1.5473... Val Loss: 1.6996\n",
      "Epoch: 27/40... Step: 3660... Loss: 1.5466... Val Loss: 1.6973\n",
      "Epoch: 27/40... Step: 3670... Loss: 1.5457... Val Loss: 1.6961\n",
      "Epoch: 27/40... Step: 3680... Loss: 1.5478... Val Loss: 1.6966\n",
      "Epoch: 27/40... Step: 3690... Loss: 1.4996... Val Loss: 1.6970\n",
      "Epoch: 27/40... Step: 3700... Loss: 1.5294... Val Loss: 1.6991\n",
      "Epoch: 27/40... Step: 3710... Loss: 1.5244... Val Loss: 1.7024\n",
      "Epoch: 27/40... Step: 3720... Loss: 1.5251... Val Loss: 1.6972\n",
      "Epoch: 27/40... Step: 3730... Loss: 1.5480... Val Loss: 1.6988\n",
      "Epoch: 27/40... Step: 3740... Loss: 1.5598... Val Loss: 1.6961\n",
      "Epoch: 27/40... Step: 3750... Loss: 1.5473... Val Loss: 1.6984\n",
      "Epoch: 28/40... Step: 3760... Loss: 1.5206... Val Loss: 1.6963\n",
      "Epoch: 28/40... Step: 3770... Loss: 1.5330... Val Loss: 1.6952\n",
      "Epoch: 28/40... Step: 3780... Loss: 1.5238... Val Loss: 1.6907\n",
      "Epoch: 28/40... Step: 3790... Loss: 1.5756... Val Loss: 1.6920\n",
      "Epoch: 28/40... Step: 3800... Loss: 1.5536... Val Loss: 1.6865\n",
      "Epoch: 28/40... Step: 3810... Loss: 1.5280... Val Loss: 1.6861\n",
      "Epoch: 28/40... Step: 3820... Loss: 1.5304... Val Loss: 1.6899\n",
      "Epoch: 28/40... Step: 3830... Loss: 1.5253... Val Loss: 1.7011\n",
      "Epoch: 28/40... Step: 3840... Loss: 1.5333... Val Loss: 1.6918\n",
      "Epoch: 28/40... Step: 3850... Loss: 1.5338... Val Loss: 1.7004\n",
      "Epoch: 28/40... Step: 3860... Loss: 1.5302... Val Loss: 1.6941\n",
      "Epoch: 28/40... Step: 3870... Loss: 1.5235... Val Loss: 1.6966\n",
      "Epoch: 28/40... Step: 3880... Loss: 1.5309... Val Loss: 1.7145\n",
      "Epoch: 28/40... Step: 3890... Loss: 1.5411... Val Loss: 1.6895\n",
      "Epoch: 29/40... Step: 3900... Loss: 1.5430... Val Loss: 1.6880\n",
      "Epoch: 29/40... Step: 3910... Loss: 1.5246... Val Loss: 1.6894\n",
      "Epoch: 29/40... Step: 3920... Loss: 1.5450... Val Loss: 1.6920\n",
      "Epoch: 29/40... Step: 3930... Loss: 1.5576... Val Loss: 1.6924\n",
      "Epoch: 29/40... Step: 3940... Loss: 1.5168... Val Loss: 1.6875\n",
      "Epoch: 29/40... Step: 3950... Loss: 1.5290... Val Loss: 1.6852\n",
      "Epoch: 29/40... Step: 3960... Loss: 1.5260... Val Loss: 1.6840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/40... Step: 3970... Loss: 1.5520... Val Loss: 1.6945\n",
      "Epoch: 29/40... Step: 3980... Loss: 1.5041... Val Loss: 1.6893\n",
      "Epoch: 29/40... Step: 3990... Loss: 1.5256... Val Loss: 1.6834\n",
      "Epoch: 29/40... Step: 4000... Loss: 1.5189... Val Loss: 1.6911\n",
      "Epoch: 29/40... Step: 4010... Loss: 1.5175... Val Loss: 1.6814\n",
      "Epoch: 29/40... Step: 4020... Loss: 1.5192... Val Loss: 1.6909\n",
      "Epoch: 29/40... Step: 4030... Loss: 1.5256... Val Loss: 1.6832\n",
      "Epoch: 30/40... Step: 4040... Loss: 1.5274... Val Loss: 1.6910\n",
      "Epoch: 30/40... Step: 4050... Loss: 1.5289... Val Loss: 1.6903\n",
      "Epoch: 30/40... Step: 4060... Loss: 1.5483... Val Loss: 1.6846\n",
      "Epoch: 30/40... Step: 4070... Loss: 1.5361... Val Loss: 1.6806\n",
      "Epoch: 30/40... Step: 4080... Loss: 1.5106... Val Loss: 1.6767\n",
      "Epoch: 30/40... Step: 4090... Loss: 1.5416... Val Loss: 1.6779\n",
      "Epoch: 30/40... Step: 4100... Loss: 1.5044... Val Loss: 1.6770\n",
      "Epoch: 30/40... Step: 4110... Loss: 1.5243... Val Loss: 1.6796\n",
      "Epoch: 30/40... Step: 4120... Loss: 1.5105... Val Loss: 1.6862\n",
      "Epoch: 30/40... Step: 4130... Loss: 1.5088... Val Loss: 1.6901\n",
      "Epoch: 30/40... Step: 4140... Loss: 1.5028... Val Loss: 1.6835\n",
      "Epoch: 30/40... Step: 4150... Loss: 1.5044... Val Loss: 1.6768\n",
      "Epoch: 30/40... Step: 4160... Loss: 1.5440... Val Loss: 1.6717\n",
      "Epoch: 30/40... Step: 4170... Loss: 1.5484... Val Loss: 1.6752\n",
      "Epoch: 31/40... Step: 4180... Loss: 1.5556... Val Loss: 1.6857\n",
      "Epoch: 31/40... Step: 4190... Loss: 1.5588... Val Loss: 1.6763\n",
      "Epoch: 31/40... Step: 4200... Loss: 1.5544... Val Loss: 1.6726\n",
      "Epoch: 31/40... Step: 4210... Loss: 1.5181... Val Loss: 1.6733\n",
      "Epoch: 31/40... Step: 4220... Loss: 1.5528... Val Loss: 1.6756\n",
      "Epoch: 31/40... Step: 4230... Loss: 1.4847... Val Loss: 1.6718\n",
      "Epoch: 31/40... Step: 4240... Loss: 1.5033... Val Loss: 1.6766\n",
      "Epoch: 31/40... Step: 4250... Loss: 1.5003... Val Loss: 1.6827\n",
      "Epoch: 31/40... Step: 4260... Loss: 1.5169... Val Loss: 1.6833\n",
      "Epoch: 31/40... Step: 4270... Loss: 1.5184... Val Loss: 1.6797\n",
      "Epoch: 31/40... Step: 4280... Loss: 1.5034... Val Loss: 1.6742\n",
      "Epoch: 31/40... Step: 4290... Loss: 1.4817... Val Loss: 1.6826\n",
      "Epoch: 31/40... Step: 4300... Loss: 1.5304... Val Loss: 1.6739\n",
      "Epoch: 32/40... Step: 4310... Loss: 1.5888... Val Loss: 1.6760\n",
      "Epoch: 32/40... Step: 4320... Loss: 1.5276... Val Loss: 1.6775\n",
      "Epoch: 32/40... Step: 4330... Loss: 1.5454... Val Loss: 1.6711\n",
      "Epoch: 32/40... Step: 4340... Loss: 1.5494... Val Loss: 1.6697\n",
      "Epoch: 32/40... Step: 4350... Loss: 1.5003... Val Loss: 1.6726\n",
      "Epoch: 32/40... Step: 4360... Loss: 1.4769... Val Loss: 1.6739\n",
      "Epoch: 32/40... Step: 4370... Loss: 1.4765... Val Loss: 1.6725\n",
      "Epoch: 32/40... Step: 4380... Loss: 1.5117... Val Loss: 1.6694\n",
      "Epoch: 32/40... Step: 4390... Loss: 1.4986... Val Loss: 1.6759\n",
      "Epoch: 32/40... Step: 4400... Loss: 1.5098... Val Loss: 1.6738\n",
      "Epoch: 32/40... Step: 4410... Loss: 1.5259... Val Loss: 1.6708\n",
      "Epoch: 32/40... Step: 4420... Loss: 1.5014... Val Loss: 1.6724\n",
      "Epoch: 32/40... Step: 4430... Loss: 1.4716... Val Loss: 1.6696\n",
      "Epoch: 32/40... Step: 4440... Loss: 1.5250... Val Loss: 1.6707\n",
      "Epoch: 33/40... Step: 4450... Loss: 1.5055... Val Loss: 1.6648\n",
      "Epoch: 33/40... Step: 4460... Loss: 1.5241... Val Loss: 1.6725\n",
      "Epoch: 33/40... Step: 4470... Loss: 1.4961... Val Loss: 1.6615\n",
      "Epoch: 33/40... Step: 4480... Loss: 1.5045... Val Loss: 1.6639\n",
      "Epoch: 33/40... Step: 4490... Loss: 1.4698... Val Loss: 1.6676\n",
      "Epoch: 33/40... Step: 4500... Loss: 1.4958... Val Loss: 1.6660\n",
      "Epoch: 33/40... Step: 4510... Loss: 1.5207... Val Loss: 1.6650\n",
      "Epoch: 33/40... Step: 4520... Loss: 1.4954... Val Loss: 1.6648\n",
      "Epoch: 33/40... Step: 4530... Loss: 1.4746... Val Loss: 1.6685\n",
      "Epoch: 33/40... Step: 4540... Loss: 1.5067... Val Loss: 1.6682\n",
      "Epoch: 33/40... Step: 4550... Loss: 1.5141... Val Loss: 1.6659\n",
      "Epoch: 33/40... Step: 4560... Loss: 1.4814... Val Loss: 1.6658\n",
      "Epoch: 33/40... Step: 4570... Loss: 1.4890... Val Loss: 1.6662\n",
      "Epoch: 33/40... Step: 4580... Loss: 1.5108... Val Loss: 1.6659\n",
      "Epoch: 34/40... Step: 4590... Loss: 1.5139... Val Loss: 1.6614\n",
      "Epoch: 34/40... Step: 4600... Loss: 1.5063... Val Loss: 1.6670\n",
      "Epoch: 34/40... Step: 4610... Loss: 1.5101... Val Loss: 1.6566\n",
      "Epoch: 34/40... Step: 4620... Loss: 1.4592... Val Loss: 1.6615\n",
      "Epoch: 34/40... Step: 4630... Loss: 1.4551... Val Loss: 1.6656\n",
      "Epoch: 34/40... Step: 4640... Loss: 1.5179... Val Loss: 1.6647\n",
      "Epoch: 34/40... Step: 4650... Loss: 1.5077... Val Loss: 1.6645\n",
      "Epoch: 34/40... Step: 4660... Loss: 1.5077... Val Loss: 1.6620\n",
      "Epoch: 34/40... Step: 4670... Loss: 1.5218... Val Loss: 1.6606\n",
      "Epoch: 34/40... Step: 4680... Loss: 1.5002... Val Loss: 1.6594\n",
      "Epoch: 34/40... Step: 4690... Loss: 1.5143... Val Loss: 1.6579\n",
      "Epoch: 34/40... Step: 4700... Loss: 1.4947... Val Loss: 1.6614\n",
      "Epoch: 34/40... Step: 4710... Loss: 1.4598... Val Loss: 1.6619\n",
      "Epoch: 34/40... Step: 4720... Loss: 1.5367... Val Loss: 1.6580\n",
      "Epoch: 35/40... Step: 4730... Loss: 1.4764... Val Loss: 1.6618\n",
      "Epoch: 35/40... Step: 4740... Loss: 1.4820... Val Loss: 1.6612\n",
      "Epoch: 35/40... Step: 4750... Loss: 1.4762... Val Loss: 1.6590\n",
      "Epoch: 35/40... Step: 4760... Loss: 1.4798... Val Loss: 1.6607\n",
      "Epoch: 35/40... Step: 4770... Loss: 1.4813... Val Loss: 1.6558\n",
      "Epoch: 35/40... Step: 4780... Loss: 1.4667... Val Loss: 1.6537\n",
      "Epoch: 35/40... Step: 4790... Loss: 1.4931... Val Loss: 1.6586\n",
      "Epoch: 35/40... Step: 4800... Loss: 1.5158... Val Loss: 1.6575\n",
      "Epoch: 35/40... Step: 4810... Loss: 1.4805... Val Loss: 1.6602\n",
      "Epoch: 35/40... Step: 4820... Loss: 1.4947... Val Loss: 1.6561\n",
      "Epoch: 35/40... Step: 4830... Loss: 1.4807... Val Loss: 1.6548\n",
      "Epoch: 35/40... Step: 4840... Loss: 1.5058... Val Loss: 1.6538\n",
      "Epoch: 35/40... Step: 4850... Loss: 1.5082... Val Loss: 1.6642\n",
      "Epoch: 35/40... Step: 4860... Loss: 1.4906... Val Loss: 1.6599\n",
      "Epoch: 36/40... Step: 4870... Loss: 1.4998... Val Loss: 1.6573\n",
      "Epoch: 36/40... Step: 4880... Loss: 1.4883... Val Loss: 1.6565\n",
      "Epoch: 36/40... Step: 4890... Loss: 1.4663... Val Loss: 1.6507\n",
      "Epoch: 36/40... Step: 4900... Loss: 1.4932... Val Loss: 1.6525\n",
      "Epoch: 36/40... Step: 4910... Loss: 1.4669... Val Loss: 1.6560\n",
      "Epoch: 36/40... Step: 4920... Loss: 1.4636... Val Loss: 1.6532\n",
      "Epoch: 36/40... Step: 4930... Loss: 1.4988... Val Loss: 1.6529\n",
      "Epoch: 36/40... Step: 4940... Loss: 1.4810... Val Loss: 1.6567\n",
      "Epoch: 36/40... Step: 4950... Loss: 1.4816... Val Loss: 1.6527\n",
      "Epoch: 36/40... Step: 4960... Loss: 1.4683... Val Loss: 1.6556\n",
      "Epoch: 36/40... Step: 4970... Loss: 1.4965... Val Loss: 1.6564\n",
      "Epoch: 36/40... Step: 4980... Loss: 1.4767... Val Loss: 1.6518\n",
      "Epoch: 36/40... Step: 4990... Loss: 1.4354... Val Loss: 1.6561\n",
      "Epoch: 36/40... Step: 5000... Loss: 1.4951... Val Loss: 1.6562\n",
      "Epoch: 37/40... Step: 5010... Loss: 1.4519... Val Loss: 1.6524\n",
      "Epoch: 37/40... Step: 5020... Loss: 1.4704... Val Loss: 1.6572\n",
      "Epoch: 37/40... Step: 5030... Loss: 1.4679... Val Loss: 1.6485\n",
      "Epoch: 37/40... Step: 5040... Loss: 1.4762... Val Loss: 1.6510\n",
      "Epoch: 37/40... Step: 5050... Loss: 1.4819... Val Loss: 1.6484\n",
      "Epoch: 37/40... Step: 5060... Loss: 1.4859... Val Loss: 1.6519\n",
      "Epoch: 37/40... Step: 5070... Loss: 1.4836... Val Loss: 1.6473\n",
      "Epoch: 37/40... Step: 5080... Loss: 1.4471... Val Loss: 1.6504\n",
      "Epoch: 37/40... Step: 5090... Loss: 1.4816... Val Loss: 1.6549\n",
      "Epoch: 37/40... Step: 5100... Loss: 1.4628... Val Loss: 1.6494\n",
      "Epoch: 37/40... Step: 5110... Loss: 1.4612... Val Loss: 1.6500\n",
      "Epoch: 37/40... Step: 5120... Loss: 1.4815... Val Loss: 1.6480\n",
      "Epoch: 37/40... Step: 5130... Loss: 1.4897... Val Loss: 1.6559\n",
      "Epoch: 37/40... Step: 5140... Loss: 1.5028... Val Loss: 1.6589\n",
      "Epoch: 38/40... Step: 5150... Loss: 1.4688... Val Loss: 1.6506\n",
      "Epoch: 38/40... Step: 5160... Loss: 1.4734... Val Loss: 1.6484\n",
      "Epoch: 38/40... Step: 5170... Loss: 1.4648... Val Loss: 1.6439\n",
      "Epoch: 38/40... Step: 5180... Loss: 1.5111... Val Loss: 1.6486\n",
      "Epoch: 38/40... Step: 5190... Loss: 1.4938... Val Loss: 1.6465\n",
      "Epoch: 38/40... Step: 5200... Loss: 1.4702... Val Loss: 1.6440\n",
      "Epoch: 38/40... Step: 5210... Loss: 1.4751... Val Loss: 1.6406\n",
      "Epoch: 38/40... Step: 5220... Loss: 1.4633... Val Loss: 1.6469\n",
      "Epoch: 38/40... Step: 5230... Loss: 1.4673... Val Loss: 1.6462\n",
      "Epoch: 38/40... Step: 5240... Loss: 1.4718... Val Loss: 1.6458\n",
      "Epoch: 38/40... Step: 5250... Loss: 1.4733... Val Loss: 1.6444\n",
      "Epoch: 38/40... Step: 5260... Loss: 1.4583... Val Loss: 1.6423\n",
      "Epoch: 38/40... Step: 5270... Loss: 1.4633... Val Loss: 1.6406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/40... Step: 5280... Loss: 1.4732... Val Loss: 1.6471\n",
      "Epoch: 39/40... Step: 5290... Loss: 1.4848... Val Loss: 1.6450\n",
      "Epoch: 39/40... Step: 5300... Loss: 1.4757... Val Loss: 1.6430\n",
      "Epoch: 39/40... Step: 5310... Loss: 1.4966... Val Loss: 1.6413\n",
      "Epoch: 39/40... Step: 5320... Loss: 1.5009... Val Loss: 1.6396\n",
      "Epoch: 39/40... Step: 5330... Loss: 1.4548... Val Loss: 1.6374\n",
      "Epoch: 39/40... Step: 5340... Loss: 1.4699... Val Loss: 1.6363\n",
      "Epoch: 39/40... Step: 5350... Loss: 1.4634... Val Loss: 1.6376\n",
      "Epoch: 39/40... Step: 5360... Loss: 1.5054... Val Loss: 1.6498\n",
      "Epoch: 39/40... Step: 5370... Loss: 1.4537... Val Loss: 1.6449\n",
      "Epoch: 39/40... Step: 5380... Loss: 1.4613... Val Loss: 1.6440\n",
      "Epoch: 39/40... Step: 5390... Loss: 1.4502... Val Loss: 1.6429\n",
      "Epoch: 39/40... Step: 5400... Loss: 1.4546... Val Loss: 1.6414\n",
      "Epoch: 39/40... Step: 5410... Loss: 1.4628... Val Loss: 1.6436\n",
      "Epoch: 39/40... Step: 5420... Loss: 1.4604... Val Loss: 1.6417\n",
      "Epoch: 40/40... Step: 5430... Loss: 1.4751... Val Loss: 1.6437\n",
      "Epoch: 40/40... Step: 5440... Loss: 1.4705... Val Loss: 1.6420\n",
      "Epoch: 40/40... Step: 5450... Loss: 1.4866... Val Loss: 1.6395\n",
      "Epoch: 40/40... Step: 5460... Loss: 1.4854... Val Loss: 1.6387\n",
      "Epoch: 40/40... Step: 5470... Loss: 1.4672... Val Loss: 1.6368\n",
      "Epoch: 40/40... Step: 5480... Loss: 1.4613... Val Loss: 1.6337\n",
      "Epoch: 40/40... Step: 5490... Loss: 1.4449... Val Loss: 1.6316\n",
      "Epoch: 40/40... Step: 5500... Loss: 1.4610... Val Loss: 1.6332\n",
      "Epoch: 40/40... Step: 5510... Loss: 1.4533... Val Loss: 1.6342\n",
      "Epoch: 40/40... Step: 5520... Loss: 1.4423... Val Loss: 1.6383\n",
      "Epoch: 40/40... Step: 5530... Loss: 1.4430... Val Loss: 1.6396\n",
      "Epoch: 40/40... Step: 5540... Loss: 1.4453... Val Loss: 1.6370\n",
      "Epoch: 40/40... Step: 5550... Loss: 1.4784... Val Loss: 1.6362\n",
      "Epoch: 40/40... Step: 5560... Loss: 1.4923... Val Loss: 1.6377\n"
     ]
    }
   ],
   "source": [
    "no_of_seqs_per_batch, no_of_timesteps_per_sequence = 128, 100\n",
    "#set to training mode\n",
    "network.train()\n",
    "train(network, each_char_in_encodeed_integer_form, epochs= 40, no_of_seqs_per_batch=no_of_seqs_per_batch, no_of_timesteps_per_sequence=no_of_timesteps_per_sequence, lr=0.0001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "In defining the model:\n",
    "* `n_hidden` - The number of units in the hidden layers.\n",
    "* `n_layers` - Number of hidden LSTM layers to use.\n",
    "\n",
    "We assume that dropout probability and learning rate will be kept at the default, in this example.\n",
    "\n",
    "And in training:\n",
    "* `no_of_seqs_per_batch` - Number of sequences running through the network in one pass.\n",
    "* `no_of_timesteps_per_sequence` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lr` - Learning rate for training\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `n_hidden` and `n_layers`. I would advise that you always use `n_layers` of either 2/3. The `n_hidden` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `n_hidden` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'lstm_epoch.network'\n",
    "checkpoint = {'no_of_hidden_nodes_in_each_lstm_cell': network.no_of_hidden_nodes_in_each_lstm_cell,\n",
    "              'no_of_stacked_lstm_layers': network.no_of_stacked_lstm_layers,\n",
    "              'state_dict': network.state_dict(),\n",
    "              'tokens': network.character_vocabulary}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it. To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorcial probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(trained_network_which_will_sample, no_of_characters_to_generate_in_the_sequence, \n",
    "           initilaization_characters_instead_of_random_start='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        trained_network_which_will_sample.cuda()\n",
    "    else:\n",
    "        trained_network_which_will_sample.cpu()\n",
    "\n",
    "    #work on evaluation mode    \n",
    "    trained_network_which_will_sample.eval()\n",
    "    \n",
    "    # First off, run through the prime/ non-random characters\n",
    "    characters_list = [ch for ch in initilaization_characters_instead_of_random_start]\n",
    "    \n",
    "    #print(characters_list)\n",
    "#     import time\n",
    "#     time.sleep(333)\n",
    "    \n",
    "    #In RNN, you learn the weights for differnet nodes, however, you do not learn the states. Hence, even during\n",
    "    #evaluation, you have to initialize the hidden and cell states.\n",
    "    initialization_of_hidden_state = trained_network_which_will_sample.init_hidden(1)\n",
    "    \n",
    "    for character in initilaization_characters_instead_of_random_start:\n",
    "        character, hidden_state = trained_network_which_will_sample.predict(character, initialization_of_hidden_state, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    characters_list.append(character)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for index in range(no_of_characters_to_generate_in_the_sequence):\n",
    "        \n",
    "        character, hidden_state = trained_network_which_will_sample.predict(characters_list[-1], hidden_state, cuda=cuda, top_k=top_k)\n",
    "        characters_list.append(character)\n",
    "\n",
    "    return ''.join(characters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hed how he had as the more assited times were at the talls to her, and the pace of his\n",
      "husband a stall for his figures of the siren shopters, all what he treated\n",
      "the saying out of the courte of the passed and thing to have becouse seen\n",
      "a look as shall to a cheets and way to be a long atorather at a serfor than it\n",
      "and walked out than a canneated was to saying them to shoot or him it, as\n",
      "he seen it.\n",
      "\n",
      "\"Yes, I can to say, you wouldn't like that is now to the same,\n",
      "that this was all to say.\n",
      "\n",
      "\"I am son in all the past in it's the comportary out of his hand all,\n",
      "the day, brother in the came that which who cried the don't come of her\n",
      "took.\"\n",
      "\n",
      "\"Oh, yes she won't in the stard.\" \"I am not think to have this somith\n",
      "to starr and a mother to the stince--hore that shatisfacte to same to\n",
      "make himself indeed, he had not a conversation with whise that that\n",
      "it was a great to mile to the come what see as all his sigh in the\n",
      "portity, whom a serming he did not can to beer at a line. The stood, she\n",
      "was stardly or his better of the didered which had been sore and his head, and\n",
      "he wonled hopes. Though the pass it was now along his bit and ask her and he\n",
      "surdering her husband and she was not both and a give might her that he\n",
      "did not sat thes winlere to the some out of alm staired in him. The place at she\n",
      "was that his seef try to to happiness as to be a croal went in his ware as\n",
      "all a good, as at they somplining that is it was alone when the forterend\n",
      "and shool had asking in the seader the temest was a sort for her to the\n",
      "somal of his fare in happeness and so way somitial of all.\n",
      "\n",
      "And at which he saw, and that so to cleat of the second in hus that had\n",
      "been stalding on which his friend husbed, he despired to say, but the peciliat\n",
      "of the words, toon hereed for her.\n",
      "\n",
      "\"You want to the sama, the comman what he did not be immedrettly bren all\n",
      "some shill of it.\n",
      "\n",
      "All his fromething with the choldentered hand on her as sitcle it\n",
      "suddenly coller to the bids, the same of the memoning as stall in the send an\n"
     ]
    }
   ],
   "source": [
    "print(sample(network, 2000, initilaization_characters_instead_of_random_start='He', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 1 epoch `rnn_1_epoch.net`\n",
    "with open('lstm_epoch.network', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = LSTM_Char_Generation(checkpoint['tokens'], no_of_hidden_nodes_in_each_lstm_cell=checkpoint['no_of_hidden_nodes_in_each_lstm_cell'], no_of_stacked_lstm_layers=checkpoint['no_of_stacked_lstm_layers'])\n",
    "\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin saides. I don't have meant. They took happened, and that she was to all themether,\n",
      "but it. I was driven, and stond an his mother. So this whole truing, and\n",
      "when the down that the mean was nought to bread in a love take, he was\n",
      "stinging over the pertom of at a musiman to brought of a gonerness, and\n",
      "sometement with the provinces of his firs and the mones, as a little seemed to\n",
      "bring is serelity. This is stire, better to mears when the start of the\n",
      "could of at in itso and the stoom of a came from. Ther heart of has\n",
      "to be and that he had been supping of the sinders, as to sated on horse\n",
      "to be to been time it that he would be shoot at those that he was to that\n",
      "was too half-heart, but with a letter whise sore of it....\n",
      "\n",
      "\"The marre and imaginal, I don't think to him.\"\n",
      "\n",
      "\"I come not is!\"\n",
      "\n",
      "And that he was somptery and hearthy and a grass all the positeds of the\n",
      "pointicully shearing to them, the children to time an the same, the which\n",
      "he could not trouble her fash of\n",
      "thic amariag of the plaisting officular things, and as he\n",
      "said, she was thinks and somity or hig own a mad to this soun was signing\n",
      "and alone and that him wonder were staying them his hands tisterst or she\n",
      "with her at the clover, she was everything, she house in\n",
      "the coldress and her hands that he wanted at him, and her so suched and\n",
      "compost the morther to shants and head him with his. The whole told\n",
      "of the profess transen them the said and strick tales and so hand ought to him\n",
      "to the same works would stoples, he could not her subler of the comman,\n",
      "and then seemed, though his fealien what the countes to talk that it\n",
      "selfor her, and stone it, better tratical to tabe through her and\n",
      "his from white they wanted to bary as itself to that it to be to be\n",
      "did not she saw it one what see in that would some a stolder and the\n",
      "coldrom the parest of the portaraty fan ofe the correrations that it, he\n",
      "was strained a states, the sense of the starre and tears, and she saw al\n",
      "headth of the sent of the pare, and which had no sere of anyth\n"
     ]
    }
   ],
   "source": [
    "# Change cuda to True if you are using GPU!\n",
    "print(sample(loaded, 2000, cuda=True, top_k=5, initilaization_characters_instead_of_random_start=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
